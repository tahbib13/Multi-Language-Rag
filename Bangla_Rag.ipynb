{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "ApzDajflKaUw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load PDF (using Langchain's PDFLoader)\n"
      ],
      "metadata": {
        "id": "PG1EwdOTKj_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_pdf_langchain(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text = \"\"\n",
        "    for doc in documents:\n",
        "        text += doc.page_content + \"\\n\"\n",
        "    print(\"‚úÖ PDF loaded successfully using Langchain. Characters:\", len(text))\n",
        "    return text"
      ],
      "metadata": {
        "id": "xd6gzx_9KlDn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Split into chunks (using Langchain's RecursiveCharacterTextSplitter)"
      ],
      "metadata": {
        "id": "a7Hp5QUoKtQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_langchain(text, chunk_size=500, overlap=100):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    print(f\"Generated {len(chunks)} text chunks using Langchain splitter\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "oxUD9KLyKuSH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Embeddings (using HuggingFaceEmbeddings)"
      ],
      "metadata": {
        "id": "zz4MlLIDK07n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_model():\n",
        "    return HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "aKiBKvs0K2N3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Build FAISS index (using Langchain's FAISS wrapper)"
      ],
      "metadata": {
        "id": "HgCL1-QNK6I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_faiss_index(chunks, embeddings_model):\n",
        "    vectorstore = FAISS.from_texts(chunks, embeddings_model)\n",
        "    print(\"‚úÖ FAISS index built successfully.\")\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "LJhx1T3kLAfv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Initialize the QA pipeline\n"
      ],
      "metadata": {
        "id": "oX-JNaoTLG5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qa_pipeline():\n",
        "    model_name = \"sagorsarker/mbert-bengali-tydiqa-qa\"  # Specialized for Bengali QA\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    return pipeline(\n",
        "        \"question-answering\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "        max_seq_len=512, # Explicitly set max sequence length\n",
        "        max_question_len=64,\n",
        "        doc_stride=128\n",
        "    )"
      ],
      "metadata": {
        "id": "B4VmTTtwLIXw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Retrieve relevant documents using the vectorstore"
      ],
      "metadata": {
        "id": "9M4ZdCfBLO9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(vectorstore, query, k=15):\n",
        "    return vectorstore.similarity_search(query, k=k)"
      ],
      "metadata": {
        "id": "0K2PrXtdLQbQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Generate answer using the QA pipeline with explicit context\n"
      ],
      "metadata": {
        "id": "VLBJD55SQfGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_pipeline(qa_pipeline, documents, question):\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "    # Pass the context and question in the format expected by the pipeline\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    return result['answer']"
      ],
      "metadata": {
        "id": "c5bUOUP8QgZf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---- MAIN WORKFLOW ----\n"
      ],
      "metadata": {
        "id": "LPXC8xrMLZVf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d696daf3",
        "outputId": "c8b1eaa1-ba30-4e7f-abe9-7cd84acd2321"
      },
      "source": [
        "%pip install pypdf"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/309.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m307.2/309.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare PDF\n",
        "    pdf_path = \"Updated_HSC26-Bangla1st-Paper.pdf\"\n",
        "    text = load_pdf_langchain(pdf_path)\n",
        "    chunks = split_text_langchain(text)\n",
        "\n",
        "    # Get embeddings and build index\n",
        "    embeddings_model = get_embeddings_model()\n",
        "    vectorstore = build_faiss_index(chunks, embeddings_model)\n",
        "\n",
        "    # Initialize QA pipeline\n",
        "    qa_pipeline = get_qa_pipeline()\n",
        "\n",
        "\n",
        "    # Ask question in Bangla\n",
        "    question = \"‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\"\n",
        "\n",
        "    # Retrieve relevant documents\n",
        "    retrieved_docs = retrieve_documents(vectorstore, question)\n",
        "\n",
        "    # Generate answer using the pipeline\n",
        "    answer = generate_answer_with_pipeline(qa_pipeline, retrieved_docs, question)\n",
        "\n",
        "    print(f\"\\n‚ùì Question: {question}\")\n",
        "    print(f\"üü¢ Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydg83DpaeaoM",
        "outputId": "0c7ca6bf-f600-46aa-e0ed-39847b792387"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PDF loaded successfully using Langchain. Characters: 88602\n",
            "Generated 224 text chunks using Langchain splitter\n",
            "‚úÖ FAISS index built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Question: ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\n",
            "üü¢ Answer: ‡¶¨‡¶ø‡¶≤? \n",
            "(‡¶ï) ‡ß®‡ßß ‡¶¨‡ßç‡¶ø‡¶ø\n"
          ]
        }
      ]
    }
  ]
}